{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Kolmogorov-Arnold Network (KAN) for Lithology Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imodelsx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import set_option\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "from imodelsx import KANClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "dataset = '/content/MLGeo/KAN/feature_vectors_preprocessed.csv'\n",
    "training_data = pd.read_csv(dataset)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "feature_vectors = training_data.iloc[:,:-1]\n",
    "correct_facies_labels = training_data.iloc[:,-1]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vectors)\n",
    "scaled_features = scaler.transform(feature_vectors)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, correct_facies_labels, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train KAN on our training data. Like you know in ML and neural networks, there are a few parameters of KAN we should aware of:\n",
    "\n",
    "* Model hyperparameters: hidden_layer_size, regularize_activation, regularize_entropy, regularize_ridge\n",
    "* Training configuration: batch_size, lr (learning rate), weight_decay, gamma\n",
    "\n",
    "We will use default parameters and see how accurate it is. We will use weighted F1-score metric to evaluate on training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = KANClassifier(device='cuda')\n",
    "\n",
    "# Fit with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate F1 accuracy on train set\n",
    "y_train_pred = model.predict(X_train)\n",
    "f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "print(f1_train)\n",
    "\n",
    "# Evaluate F1 accuracy on test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first result we have F1-score training 0.29 and F1-score testing 0.32. Definitely, not accurate! Let's try hyperparameter tuning. Since we have 2 categories of parameters (listed above), we try first to tune model hyperparameters. \n",
    "Currently, the imodelsx doesn't provide hyperparameter tuning like in scikit-learn. So, let's build a grid search from scratch using simple for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "layers = [256, 512, 1024]\n",
    "activation = [0.4, 0.5, 0.6]\n",
    "entropy = [0.4, 0.5, 0.6]\n",
    "ridge = [0.05, 0.1, 0.2]\n",
    "\n",
    "# varying hyperparameter\n",
    "for hidden_layer_size in layers:\n",
    "  for regularize_activation in activation:\n",
    "    for regularize_entropy in entropy:\n",
    "      for regularize_ridge in ridge:\n",
    "        model = KANClassifier(hidden_layer_size=hidden_layer_size, device='cuda',\n",
    "                                      regularize_activation=regularize_activation,\n",
    "                                      regularize_entropy=regularize_entropy,\n",
    "                                       regularize_ridge=regularize_ridge)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "        print(hidden_layer_size, regularize_activation, regularize_entropy, regularize_ridge, f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this grid search, we can manage to improve the F1-score training to 0.48 and the F1-score test to 0.50, using the following hyperparameters\n",
    "hidden_layer_size: 1024\n",
    "regularize_activation: 0.5\n",
    "regularize_entropy: 0.4\n",
    "regularize_ridge: 0.2\n",
    "An improvement of 60.7% in the F1-score compared to the default setup, but the score is still low! Remember, we still have another parameter to tune, which is the training configuration parameters. \n",
    "So, let's build another grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of training configuration\n",
    "batch = [64, 128, 512]\n",
    "learning = [0.05, 0.07, 0.1]\n",
    "weights = [0.005, 0.01, 0.03]\n",
    "gammas = [0.4, 0.5, 0.6]\n",
    "\n",
    "# Use tuned hyperparameter\n",
    "model = KANClassifier(hidden_layer_size=1024, regularize_activation=.5,\n",
    "                      regularize_entropy=.4, regularize_ridge=.2,\n",
    "                      device='cuda')\n",
    "\n",
    "# Varying training configuration\n",
    "for batch_size in batch:\n",
    "  for lr in learning:\n",
    "    for weight_decay in weights:\n",
    "      for gamma in gammas:\n",
    "        model.fit(X_train, y_train, batch_size=batch_size, lr=lr,\n",
    "                  weight_decay=weight_decay, gamma=gamma)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "        print(batch_size, lr, weight_decay, gamma, f1_train, f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this second grid search, finally, we manage to improve the F1-score training to 0.58 and the F1-score test to 0.60, using the following hyperparameters\n",
    "batch_size: 64\n",
    "lr: 0.07\n",
    "weight_decay: 0.01\n",
    "gamma: 0.6\n",
    "It is a 92.9% improvement in F1-score compared to default setup. Neat!!\n",
    "Now, we have a final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final KAN model\n",
    "model = KANClassifier(hidden_layer_size=1024, regularize_activation=.5,\n",
    "                      regularize_entropy=.4, regularize_ridge=.2,\n",
    "                      device='cuda')\n",
    "\n",
    "# Fit with training data\n",
    "model.fit(X_train, y_train, batch_size=64, lr=.07,\n",
    "                  weight_decay=.01, gamma=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
