{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from imodelsx.kan.kan_modules import KANModule, KANGAMModule\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KANLinearModule(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinearModule, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(\n",
    "            self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1,\n",
    "                               self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order: -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(\n",
    "                    self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1:] - x)\n",
    "                / (grid[:, k + 1:] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        return base_output + spline_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(\n",
    "            splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] +\n",
    "                        2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + \\\n",
    "            (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1,\n",
    "                               device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1,\n",
    "                               device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(\n",
    "            self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KANModule(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANModule, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinearModule(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(\n",
    "                regularize_activation, regularize_entropy)\n",
    "            for layer in self.layers\n",
    "        )\n",
    "\n",
    "\n",
    "class KANGAMModule(torch.nn.Module):\n",
    "    '''Learn a KAN model on each individual input feature\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_features, layers_hidden: List[int], n_classes, **kwargs):\n",
    "        super(KANGAMModule, self).__init__()\n",
    "        self.models = torch.nn.ModuleList([\n",
    "            KANModule(\n",
    "                layers_hidden=[1] + layers_hidden + [1],\n",
    "                **kwargs)\n",
    "            for _ in range(num_features)\n",
    "        ])\n",
    "        self.linear = torch.nn.Linear(num_features, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "\n",
    "        features = torch.stack(\n",
    "            [model(x[:, i:i + 1], update_grid)\n",
    "             for i, model in enumerate(self.models)],\n",
    "            dim=1)\n",
    "\n",
    "        features = features.view(x.size(0), -1)\n",
    "        return self.linear(features)\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0, regularize_ridge=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(\n",
    "                regularize_activation, regularize_entropy)\n",
    "            for model in self.models\n",
    "            for layer in model.layers\n",
    "        ) + regularize_ridge * self.linear.weight.norm(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 hidden_layer_size: int = 64,\n",
    "                 hidden_layer_sizes: List[int] = None,\n",
    "                 regularize_activation: float = 1.0, regularize_entropy: float = 1.0, regularize_ridge: float = 0.0,\n",
    "                 test_size=0.2, random_state=42, shuffle=True,\n",
    "                 device: str = 'cpu',\n",
    "                 **kwargs):\n",
    "        '''\n",
    "        Params\n",
    "        ------\n",
    "        hidden_layer_size : int\n",
    "            If int, number of neurons in the hidden layer (assumes single hidden layer)\n",
    "        hidden_layer_sizes: List with length (n_layers - 2)\n",
    "            The ith element represents the number of neurons in the ith hidden layer.\n",
    "            If this is passed, will override hidden_layer_size\n",
    "            e.g. [32, 64] would have a layer with 32 hidden units followed by a layer with 64 hidden units\n",
    "            (input and output shape are inferred by the data passed)\n",
    "        regularize_activation: float\n",
    "            Activation regularization parameter\n",
    "        regularize_entropy: float\n",
    "            Entropy regularization parameter\n",
    "        regularize_ridge: float\n",
    "            Ridge regularization parameter (only applies to KANGAM)\n",
    "        kwargs can be any of these more detailed KAN parameters\n",
    "            grid_size=5,\n",
    "            spline_order=3,\n",
    "            scale_noise=0.1,\n",
    "            scale_base=1.0,\n",
    "            scale_spline=1.0,\n",
    "            base_activation=torch.nn.SiLU,\n",
    "            grid_eps=0.02,\n",
    "            grid_range=[-1, 1],\n",
    "        '''\n",
    "        if hidden_layer_sizes is not None:\n",
    "            self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        else:\n",
    "            self.hidden_layer_sizes = [hidden_layer_size]\n",
    "        self.device = device\n",
    "        self.regularize_activation = regularize_activation\n",
    "        self.regularize_entropy = regularize_entropy\n",
    "        self.regularize_ridge = regularize_ridge\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y, batch_size=512, lr=1e-3, weight_decay=1e-4, gamma=0.8):\n",
    "        if isinstance(self, ClassifierMixin):\n",
    "            check_classification_targets(y)\n",
    "            self.classes_, y = np.unique(y, return_inverse=True)\n",
    "            num_outputs = len(self.classes_)\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "        else:\n",
    "            num_outputs = 1\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        if isinstance(self, (KANGAMClassifier, KANGAMRegressor)):\n",
    "            self.model = KANGAMModule(\n",
    "                num_features=num_features,\n",
    "                layers_hidden=self.hidden_layer_sizes,\n",
    "                n_classes=num_outputs,\n",
    "                **self.kwargs\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.model = KANModule(\n",
    "                layers_hidden=[num_features] +\n",
    "                self.hidden_layer_sizes + [num_outputs],\n",
    "            ).to(self.device)\n",
    "\n",
    "        X_train, X_tune, y_train, y_tune = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "        dset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        dset_tune = torch.utils.data.TensorDataset(X_tune, y_tune)\n",
    "        loader_train = DataLoader(\n",
    "            dset_train, batch_size=batch_size, shuffle=True)\n",
    "        loader_tune = DataLoader(\n",
    "            dset_tune, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = optim.AdamW(self.model.parameters(),\n",
    "                                lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "        # Define loss\n",
    "        if isinstance(self, ClassifierMixin):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        tune_losses = []\n",
    "        for epoch in tqdm(range(100)):\n",
    "            self.model.train()\n",
    "            for x, labs in loader_train:\n",
    "                x = x.view(-1, num_features).to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(x).squeeze()\n",
    "                loss = criterion(output, labs.to(self.device).squeeze())\n",
    "                if isinstance(self, (KANGAMClassifier, KANGAMRegressor)):\n",
    "                    loss += self.model.regularization_loss(\n",
    "                        self.regularize_activation, self.regularize_entropy, self.regularize_ridge)\n",
    "                else:\n",
    "                    loss += self.model.regularization_loss(\n",
    "                        self.regularize_activation, self.regularize_entropy)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            tune_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x, labs in loader_tune:\n",
    "                    x = x.view(-1, num_features).to(self.device)\n",
    "                    output = self.model(x).squeeze()\n",
    "                    tune_loss += criterion(output,\n",
    "                                           labs.to(self.device).squeeze()).item()\n",
    "            tune_loss /= len(loader_tune)\n",
    "            tune_losses.append(tune_loss)\n",
    "            scheduler.step()\n",
    "\n",
    "            # apply early stopping\n",
    "            if len(tune_losses) > 3 and tune_losses[-1] > tune_losses[-2]:\n",
    "                print(\"\\tEarly stopping\")\n",
    "                return self\n",
    "\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        output = self.model(X)\n",
    "        if isinstance(self, ClassifierMixin):\n",
    "            return self.classes_[output.argmax(dim=1).cpu().numpy()]\n",
    "        else:\n",
    "            return output.cpu().numpy()\n",
    "\n",
    "\n",
    "class KANClassifier(KAN, ClassifierMixin):\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        output = self.model(X)\n",
    "        return torch.nn.functional.softmax(output, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "class KANRegressor(KAN, RegressorMixin):\n",
    "    pass\n",
    "\n",
    "\n",
    "class KANGAMClassifier(KANClassifier):\n",
    "    pass\n",
    "\n",
    "\n",
    "class KANGAMRegressor(KANRegressor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 41.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=8, n_informative=2)\n",
    "\n",
    "\n",
    "model = KANClassifier(hidden_layer_size=64, device='cpu', regularize_activation=1.0, regularize_entropy=1.0)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "print('Test acc', accuracy_score(y, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
